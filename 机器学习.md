# 机器学习

## 统计学习概论

统计学习，也称统计机器学习
目的：用于对数据的预测与分析
方法：统计学习由监督学习，无监督学习，强化学习组成
方法的三要素：模型、策略和算法

### 分类

基本分类：

1. 监督学习
2. 无监督学习
3. 强化学习
4. 有时候也包括半监督学习和主动学习

按模型分类：

- 概率模型与非概率模型
  概率模型取条件概率分布形式，非概率模型取函数形式
  在监督学习中概率模型是生成模型，非概率模型是判别模型
- 非概率模型又可以分为：线性模型与非线性模型
- 参数化模型和非参数化模型
  参数化模型参照模型的参数维度不变，可以由有限维数参数完全刻画，非参数模型假设模型参数的维度不固定或无限大

按算法分类：

- 在线学习
  每次只接受一个样本
- 批量学习
  一次接受所有数据

按技巧分类：

- 贝叶斯学习
- 核方法
  使用和函数表示和学习非线性模型的一种方法
  把线性模型拓展到非线性模型，直接的做法是显式地定义从输入空间（低维）到特征空间（高维）的映射，在特征空间中进行内积计算，而核方法在于不显式的定义这个映射而是直接定义核函数，即映射之后在特征空间的内积

*注：监督学习和非监督学习不是相对的，其实区别不大，主要在有无标签（正确答案）、目标任务（预测和分类\发现隐藏结构）*

### 统计学习方法的三要素

==**模型**==：在监督学习中模型就是所要学习的条件概率分布或决策函数

==**策略**==：按照什么样的准则学习或者选择最优模型，引入损失函数（度量模型一次预测的好坏）和风险函数（度量平均意义下模型的好坏）
	常用的损失函数：

1. 0-1损失函数

2. 平方损失函数

3. 绝对损失函数

4. 对数损失函数

   给定一个训练集，模型关于训练集的平均损失方程称为**经验风险**或者**经验损失**
   当样本容量N趋于无穷时**经验风险趋于期望风险**
   这又引出监督学习的两个基本策略

   - 经验风险最小化
     当样本容量足够大时，有很好的学习效果
   - 结构风险最小化
     等价于正则化，当样本容量小时，为了防止过拟合化，就是在经验风险上加一个正则化项

==**算法**==：是指学习模型具体计算方法

### 正则化与交叉验证

- L1正则化：适用于特征选择
- L2正则化：适用于防止过拟合，模型越复杂罚项越大

另一种常用的模型选择方法--==**交叉验证**==：
如果样本数据充足，进行模型选择的一种简单方法是随机将数据集切割成三部分：训练集、验证集、测试集。但是在数据不充足时可以用交叉验证

1. 简单交叉验证
   随机分两部分，训练集和测试集，训练集在各种条件下训练模型，得到不同模型，选测试误差最小的
2. S折交叉验证（应用最多）
   随机将数据集切分为S个互补相交、大小相同的子集，然后利用S-1个子集的数据训练模型，用余下的那个测试模型，将这一过程对可能的S种选择重复进行，选出S次评测中平均测试误差最小的模型
3. 留一交叉验证
   S折交叉验证的特情形S=N，N是给定的样本容量，通常在数据缺乏时使用

### 泛化能力

指由该方法学习到的模型对未知数据的预测能力

泛化误差上界：

- 泛化能力的分析往往是通过研究泛化误差的概率上界进行
- 它是样本容量的函数，样本容量增加，泛化上界趋于0
- 它是假设空间容量的函数，假设空间容量越大，模型就越难学，泛化误差上界就大

### 生成模型和判别模型

监督学习方法分为生成方法和判别方法，所学模型就称生成模型和判别模型

生成模型：核心思想试图学习联合概率分布，它能够产生新的数据样本
特点：

- 学习数据的生成过程
- 可生成新样本，适合无监督或半监督学习
- 通常需要更多数据

判别模型：核心思想直接学习条件概率或决策边界，关注如何区分不同类型的数据，而不关心数据的生成关系
特点：

- 聚焦分类/回归任务：直接建模输入X和输出Y的关系
- 无法产生新样本
- 通常更高效

### 应用

**分类问题**：

当输出变量Y取有限个离散值，预测问题便成为分类问题
包括学习和分类两个过程，根据已知训练数据集利用有效的学习方法学习一个分类器（分类模型或分类决策函数），对新输入的实例进行分类，分类器性能的评价指标一般是分类准确率，对于二分类问题，评价指标是精确率和召回率

**标注问题**：

可以认为标注问题是分类问题的一个推广，又是更复杂的结构预测问题的简单形式
输入一个观测序列，输出一个标记或状态序列（可以是离散的也可以是连续的）
常见的应用：信息抽取，自然语言处理（词性标注），检测与定位（标注对象位置）

**回归问题**：

用来预测输入变量与输出变量之间的关系，回归模型正是表示从输入变量到输出变量之间映射的函数
按照变量个数分为一元回归和多元回归，按照模型类型分为线性和非线性
回归问题可以由著名的最小二乘法求解











- 梯度下降要注意同步性
- 向量化可以让代码跟简洁运行更高效
- x下标区分特征，上标区分样本